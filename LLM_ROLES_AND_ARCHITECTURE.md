# ğŸ¤– LLM Roles and Architecture in DCS

**Document Version:** 1.0  
**Last Updated:** October 19, 2025  
**Purpose:** Comprehensive explanation of Large Language Model (LLM) roles, interactions, and governance in the Discharge Summary Generator

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [What are LLMs in DCS?](#what-are-llms-in-dcs)
3. [LLM Providers and Models](#llm-providers-and-models)
4. [Where LLMs are Used](#where-llms-are-used)
5. [LLM Integration Architecture](#llm-integration-architecture)
6. [How LLMs Interact with Other Code](#how-llms-interact-with-other-code)
7. [Governance and Control Flow](#governance-and-control-flow)
8. [LLM vs Pattern-Based Processing](#llm-vs-pattern-based-processing)
9. [Error Handling and Fallback Mechanisms](#error-handling-and-fallback-mechanisms)
10. [Cost Tracking and Monitoring](#cost-tracking-and-monitoring)
11. [Security and Privacy Considerations](#security-and-privacy-considerations)
12. [Code Examples](#code-examples)
13. [FAQ](#faq)

---

## Executive Summary

The Discharge Summary Generator (DCS) uses **Large Language Models (LLMs)** as intelligent processing engines that transform unstructured clinical notes into structured medical data and professional narratives. LLMs act as **AI assistants** that are:

- **Governed by**: Orchestration layers, validation services, and quality metrics
- **Enhanced by**: Pattern-based extraction, medical knowledge bases, and learning systems
- **Controlled by**: User preferences, confidence thresholds, and iterative refinement loops
- **Monitored by**: Cost tracking, performance metrics, and validation systems

LLMs are **NOT autonomous** - they operate within a sophisticated framework that validates their outputs, applies medical domain knowledge, and ensures accuracy through multiple layers of checks and balances.

---

## What are LLMs in DCS?

### Definition

Large Language Models (LLMs) in DCS are **cloud-based AI services** that:
- Process natural language text (clinical notes)
- Extract structured information (patient data, diagnoses, medications)
- Generate professional medical narratives (discharge summaries)
- Apply medical knowledge and reasoning

### Primary Functions

1. **Data Extraction** - Parse unstructured clinical notes into structured fields
2. **Narrative Generation** - Create professional medical writing from structured data
3. **Medical Reasoning** - Understand clinical contexts, relationships, and timelines
4. **Language Processing** - Handle medical terminology, abbreviations, and varied writing styles

### Why LLMs?

**Challenges They Solve:**
- ğŸ“ **Unstructured Data**: Clinical notes vary wildly in format and style
- ğŸ¥ **Medical Complexity**: Understanding neurosurgical terminology and context
- â° **Time Constraints**: Manual summary writing takes 30-60 minutes
- ğŸ¯ **Accuracy Requirements**: Medical documentation demands 95%+ accuracy

**Advantages:**
- 92-98% extraction accuracy (vs 85-90% with patterns alone)
- Natural language understanding of clinical context
- Adaptive learning from different writing styles
- Professional medical narrative generation

---

## LLM Providers and Models

### Supported Providers

DCS supports **three major LLM providers** with multiple model options:

#### 1. Anthropic Claude (Primary)
- **Claude 3.5 Sonnet** â­ (Recommended)
  - Best for medical text processing
  - Context: 200K tokens
  - Cost: $3/$15 per 1M tokens (input/output)
  - Speed: Medium (2-3 seconds typical)
  - Quality: Excellent
  
- **Claude 3 Opus**
  - Highest quality, slowest
  - Context: 200K tokens
  - Cost: $15/$75 per 1M tokens
  - Speed: Slow (4-6 seconds)
  - Quality: Outstanding

- **Claude 3 Haiku**
  - Fastest, most economical
  - Context: 200K tokens
  - Cost: $0.25/$1.25 per 1M tokens
  - Speed: Very Fast (1-2 seconds)
  - Quality: Good

#### 2. OpenAI GPT
- **GPT-4o** â­ (Recommended)
  - Excellent medical knowledge
  - Context: 128K tokens
  - Cost: $2.50/$10 per 1M tokens
  - Speed: Fast (2-3 seconds)
  - Quality: Excellent

- **GPT-4o Mini**
  - Fast and economical
  - Context: 128K tokens
  - Cost: $0.15/$0.60 per 1M tokens
  - Speed: Very Fast (1-2 seconds)
  - Quality: Good

- **GPT-4 Turbo**
  - High quality, slower
  - Context: 128K tokens
  - Cost: $10/$30 per 1M tokens
  - Speed: Medium (3-4 seconds)
  - Quality: Excellent

#### 3. Google Gemini
- **Gemini 1.5 Pro** â­ (Recommended)
  - Most cost-effective
  - Context: 2M tokens (largest)
  - Cost: $1.25/$5 per 1M tokens
  - Speed: Fast (2-3 seconds)
  - Quality: Very Good

- **Gemini 1.5 Flash**
  - Ultra-fast, economical
  - Context: 1M tokens
  - Cost: $0.075/$0.30 per 1M tokens
  - Speed: Very Fast (1-2 seconds)
  - Quality: Good

### Provider Selection

**Default Priority:**
1. **Claude 3.5 Sonnet** - Best overall for medical text
2. **GPT-4o** - Excellent alternative
3. **Gemini 1.5 Pro** - Most cost-effective

**User Configurable:**
- Users can select their preferred model in Settings
- Selection persists across sessions
- Automatic fallback if selected model fails

---

## Where LLMs are Used

LLMs are invoked at **specific, controlled points** in the application workflow. They do NOT run continuously or autonomously.

### 1. Phase 1: Data Extraction

**File:** `src/services/extraction.js`  
**Function:** `extractWithLLM()`  
**Purpose:** Extract structured medical data from unstructured clinical notes

**Input:**
- Raw clinical notes (text)
- Extraction targets (demographics, pathology, medications, etc.)
- Context from pattern-based pre-processing

**LLM Task:**
```
Analyze this clinical note and extract:
- Patient demographics (age, sex, MRN)
- Dates (admission, surgery, discharge)
- Primary diagnosis and pathology
- Procedures performed
- Medications (especially anticoagulation)
- Complications
- Functional scores
```

**Output:**
- Structured JSON with extracted fields
- Confidence scores per field
- Metadata about extraction method

**When Invoked:**
- User clicks "Process Notes" button
- Only if LLM is available and enabled
- Falls back to pattern-based if LLM unavailable

---

### 2. Phase 3: Narrative Generation

**File:** `src/services/narrativeEngine.js`  
**Function:** `generateSummaryWithLLM()`  
**Purpose:** Generate professional medical narratives from structured data

**Input:**
- Structured extracted data
- Clinical intelligence (timeline, treatment responses)
- Pathology-specific templates
- Writing style preferences

**LLM Task:**
```
Generate a professional discharge summary with these sections:
- Chief Complaint (1-2 sentences)
- History of Present Illness
- Hospital Course (chronological)
- Procedures Performed
- Complications and Management
- Discharge Status
- Discharge Plan and Follow-up

Use formal medical writing style, past tense, active voice.
```

**Output:**
- Complete discharge summary sections
- Professional medical narrative
- Proper chronological flow
- Appropriate medical terminology

**When Invoked:**
- User clicks "Generate Summary" button
- Only after extraction is complete
- Only if LLM is available
- Falls back to template-based if unavailable

---

### 3. Backend Proxy Routes

**File:** `backend/server.js`  
**Endpoints:**
- `/api/llm/anthropic` - Claude API proxy
- `/api/llm/openai` - GPT API proxy
- `/api/llm/gemini` - Gemini API proxy
- `/api/extract` - Combined extraction endpoint

**Purpose:**
- Secure API key management (keys never exposed to browser)
- CORS handling for cross-origin requests
- Request/response logging
- Rate limiting and abuse prevention

**LLM Communication Flow:**
```
Frontend (Browser)
    â†“
Backend Proxy Server (Port 3001)
    â†“ [Adds API Key]
LLM Provider API (Anthropic/OpenAI/Google)
    â†“ [Returns AI Response]
Backend Proxy Server
    â†“ [Removes sensitive data]
Frontend (Browser)
```

---

### 4. Backend Extraction Service

**File:** `backend/services/neurosurgeryExtractor.js`  
**Purpose:** Server-side extraction with neurosurgery-specific patterns

**Hybrid Approach:**
1. Pattern-based extraction (always runs)
2. LLM-based extraction (optional)
3. Merge results for best accuracy

**When Used:**
- Advanced extraction mode
- When backend processing requested
- For comparison and validation

---

## LLM Integration Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INTERFACE                          â”‚
â”‚         (Upload â†’ Process â†’ Review â†’ Generate)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHASE 4: ORCHESTRATOR                           â”‚
â”‚  â€¢ Workflow coordination                                     â”‚
â”‚  â€¢ Decision making (use LLM or patterns?)                    â”‚
â”‚  â€¢ Quality thresholds                                        â”‚
â”‚  â€¢ Iterative refinement                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: EXTRACTION   â”‚   â”‚ PHASE 3: NARRATIVE     â”‚
â”‚                        â”‚   â”‚ GENERATION             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Pattern-Based    â”‚ â”‚   â”‚ â”‚ Template-Based   â”‚  â”‚
â”‚  â”‚ Extraction       â”‚ â”‚   â”‚ â”‚ Generation       â”‚  â”‚
â”‚  â”‚ (Always Runs)    â”‚ â”‚   â”‚ â”‚ (Fallback)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           +            â”‚   â”‚          +             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ğŸ¤– LLM-Based     â”‚ â”‚   â”‚ â”‚ ğŸ¤– LLM-Based     â”‚  â”‚
â”‚  â”‚ Extraction       â”‚ â”‚   â”‚ â”‚ Generation       â”‚  â”‚
â”‚  â”‚ (If Available)   â”‚ â”‚   â”‚ â”‚ (If Available)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLM SERVICE LAYER                          â”‚
â”‚  â€¢ Multi-provider support (Claude, GPT, Gemini)             â”‚
â”‚  â€¢ Automatic fallback on failure                            â”‚
â”‚  â€¢ Cost tracking and monitoring                             â”‚
â”‚  â€¢ Response caching                                         â”‚
â”‚  â€¢ Rate limiting                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BACKEND PROXY SERVER                            â”‚
â”‚  â€¢ API key management (secure)                              â”‚
â”‚  â€¢ CORS handling                                            â”‚
â”‚  â€¢ Request validation                                       â”‚
â”‚  â€¢ Response sanitization                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼         â–¼        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Anthropicâ”‚ â”‚OpenAIâ”‚ â”‚Google  â”‚
   â”‚ Claude â”‚ â”‚ GPT â”‚ â”‚Gemini  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User uploads clinical notes                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Preprocessing Layer                                      â”‚
â”‚    â€¢ Text normalization                                     â”‚
â”‚    â€¢ Deduplication                                          â”‚
â”‚    â€¢ Abbreviation expansion                                 â”‚
â”‚    â€¢ Segmentation                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Orchestrator Decides: Use LLM or Patterns?              â”‚
â”‚    â”œâ”€ Check: Is LLM available?                             â”‚
â”‚    â”œâ”€ Check: User preference?                              â”‚
â”‚    â”œâ”€ Check: Note complexity?                              â”‚
â”‚    â””â”€ Decision: Hybrid, LLM-only, or Pattern-only          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                  â”‚                  â”‚
             â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4a. Pattern     â”‚  â”‚ 4b. LLM         â”‚  â”‚ 4c. Hybrid      â”‚
â”‚     Extraction  â”‚  â”‚     Extraction  â”‚  â”‚     Extraction  â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚
â”‚ Regex patterns  â”‚  â”‚ AI reasoning    â”‚  â”‚ Both methods    â”‚
â”‚ Domain rules    â”‚  â”‚ Context aware   â”‚  â”‚ Merged results  â”‚
â”‚ 85-90% accuracy â”‚  â”‚ 92-98% accuracy â”‚  â”‚ Best of both    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚                  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Validation Layer                                         â”‚
â”‚    â€¢ Completeness check                                     â”‚
â”‚    â€¢ Accuracy validation                                    â”‚
â”‚    â€¢ Consistency verification                               â”‚
â”‚    â€¢ Confidence scoring                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Quality Assessment                                       â”‚
â”‚    â”œâ”€ If Quality < Threshold: Iterate (max 2x)             â”‚
â”‚    â”œâ”€ If Quality >= Threshold: Continue                    â”‚
â”‚    â””â”€ Learn from validation errors                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. User Reviews and Corrects (Optional)                    â”‚
â”‚    â€¢ Edit any extracted fields                             â”‚
â”‚    â€¢ System learns from corrections                         â”‚
â”‚    â€¢ Updated patterns stored                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Intelligence Hub                                         â”‚
â”‚    â€¢ Build causal timeline                                  â”‚
â”‚    â€¢ Track treatment responses                              â”‚
â”‚    â€¢ Analyze functional evolution                           â”‚
â”‚    â€¢ Generate insights                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Narrative Generation                                     â”‚
â”‚    â”œâ”€ Template-based sections                              â”‚
â”‚    â”œâ”€ ğŸ¤– LLM narrative generation (if available)           â”‚
â”‚    â””â”€ Medical writing style application                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. Final Quality Check                                     â”‚
â”‚     â€¢ Narrative quality score                               â”‚
â”‚     â€¢ Completeness verification                             â”‚
â”‚     â€¢ Style conformance                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 11. Display to User                                         â”‚
â”‚     â€¢ Complete discharge summary                            â”‚
â”‚     â€¢ Export options (PDF, text)                            â”‚
â”‚     â€¢ Learning dashboard                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How LLMs Interact with Other Code

### Interaction Points

LLMs **do not** operate in isolation. They are tightly integrated with multiple system components:

#### 1. Orchestration Layer (Governs LLM Usage)

**File:** `src/services/summaryOrchestrator.js`

**Role:** Decides WHEN and HOW to use LLMs

**Responsibilities:**
- âœ… Determines if LLM should be used
- âœ… Manages LLM call timing
- âœ… Handles LLM failures gracefully
- âœ… Implements iterative refinement
- âœ… Enforces quality thresholds

**Code Flow:**
```javascript
async function orchestrateSummaryGeneration(notes, options) {
  // 1. Build context
  const context = contextProvider.buildContext(notes);
  
  // 2. DECISION: Use LLM or patterns?
  const useLLM = isLLMAvailable() && !options.forcePatterns;
  
  // 3. Extract data (with or without LLM)
  const extraction = useLLM 
    ? await extractWithLLM(notes, context)      // LLM path
    : await extractWithPatterns(notes, context); // Pattern path
  
  // 4. Validate extraction
  const validation = await validateExtraction(extraction);
  
  // 5. DECISION: Is quality good enough?
  if (validation.quality < threshold && iterations < maxIterations) {
    // Learn from errors and retry
    return await orchestrateSummaryGeneration(notes, {
      ...options,
      iteration: iterations + 1,
      feedback: validation.errors
    });
  }
  
  // 6. Generate narrative (with or without LLM)
  const narrative = useLLM
    ? await generateSummaryWithLLM(extraction)  // LLM path
    : await generateSummaryWithTemplates(extraction); // Template path
  
  return { extraction, validation, narrative };
}
```

**Key Points:**
- Orchestrator makes ALL decisions about LLM usage
- LLMs never decide when to run themselves
- Quality checks happen BEFORE proceeding
- Fallback to patterns if LLM fails

---

#### 2. Validation Service (Checks LLM Output)

**File:** `src/services/validation.js`

**Role:** Validates ALL LLM outputs before use

**Checks Performed:**
- âœ… Required fields present?
- âœ… Data types correct?
- âœ… Values within valid ranges?
- âœ… Dates logical and consistent?
- âœ… Medical relationships valid?
- âœ… Confidence scores reasonable?

**Validation Flow:**
```javascript
async function validateExtraction(extractedData) {
  const errors = [];
  const warnings = [];
  
  // Check required fields
  if (!extractedData.pathology?.primary) {
    errors.push('Missing primary diagnosis');
  }
  
  // Check date logic
  if (extractedData.dates.discharge < extractedData.dates.admission) {
    errors.push('Discharge date before admission date');
  }
  
  // Check confidence scores
  if (extractedData.confidence.overall < 0.5) {
    warnings.push('Low confidence extraction - review carefully');
  }
  
  return {
    isValid: errors.length === 0,
    errors,
    warnings,
    quality: calculateQualityScore(extractedData)
  };
}
```

**Key Points:**
- LLM outputs are NEVER used without validation
- Invalid data triggers warnings or re-extraction
- Validation prevents hallucinations from reaching users

---

#### 3. Pattern-Based Extraction (Complements LLM)

**File:** `src/services/extraction.js`

**Role:** Provides baseline extraction, fallback, and validation reference

**Hybrid Approach:**
```javascript
async function extractMedicalEntities(notes, options) {
  // ALWAYS run pattern-based extraction first
  const patternResults = await extractWithPatterns(notes);
  
  // Try LLM extraction if available
  let llmResults = null;
  if (isLLMAvailable() && !options.forcePatterns) {
    try {
      llmResults = await extractWithLLM(notes);
    } catch (error) {
      console.warn('LLM extraction failed, using patterns only');
    }
  }
  
  // Merge results (LLM takes precedence for high-confidence fields)
  return mergeExtractionResults(patternResults, llmResults);
}
```

**Merging Logic:**
```javascript
function mergeExtractionResults(patterns, llm) {
  if (!llm) return patterns;
  
  const merged = { ...patterns };
  
  // For each field, use LLM if confidence is high enough
  for (const [field, value] of Object.entries(llm)) {
    const llmConfidence = llm.confidence[field] || 0;
    const patternConfidence = patterns.confidence[field] || 0;
    
    if (llmConfidence > 0.8 || llmConfidence > patternConfidence) {
      merged[field] = value;  // Use LLM result
    }
    // Otherwise keep pattern result
  }
  
  return merged;
}
```

**Key Points:**
- Patterns provide baseline and fallback
- LLM enhances accuracy when available
- Confidence scores determine which result to use
- System NEVER depends solely on LLM

---

#### 4. Learning Engine (Improves Both LLM and Patterns)

**File:** `src/services/ml/learningEngine.js`

**Role:** Learn from user corrections to improve future extractions

**Learning Flow:**
```javascript
async function learnFromCorrection(original, corrected, context) {
  // 1. Anonymize PHI
  const anonymized = anonymizer.removePHI({ original, corrected });
  
  // 2. Extract pattern from correction
  const pattern = extractPattern(anonymized.original, anonymized.corrected);
  
  // 3. Store learned pattern
  await knowledgeBase.addPattern({
    field: pattern.field,
    regex: pattern.regex,
    context: context.pathology,
    confidence: 0.7, // Initial confidence
    learned: new Date(),
    source: 'user_correction'
  });
  
  // 4. Update statistics
  await correctionTracker.recordCorrection({
    field: pattern.field,
    wasLLM: context.extractionMethod === 'llm',
    wasPattern: context.extractionMethod === 'pattern'
  });
}
```

**Impact on Future Extractions:**
- LLM prompts include learned patterns as examples
- Pattern-based extraction gains new regex rules
- Both methods benefit from user feedback

---

#### 5. Context Provider (Informs LLM Calls)

**File:** `src/services/context/contextProvider.js`

**Role:** Build rich context to improve LLM accuracy

**Context Building:**
```javascript
function buildContext(notes) {
  return {
    // Pathology detection (helps LLM focus)
    pathology: {
      primary: detectPrimaryPathology(notes),
      secondary: detectSecondaryDiagnoses(notes)
    },
    
    // Clinical complexity (helps select appropriate model)
    clinical: {
      complexity: assessComplexity(notes),
      specialty: 'neurosurgery'
    },
    
    // Historical patterns (helps LLM learn from past)
    learned: {
      patterns: getLearned Patterns(),
      preferences: getUserPreferences()
    },
    
    // Note characteristics (helps optimize processing)
    noteMetadata: {
      length: notes.length,
      style: detectWritingStyle(notes),
      quality: assessNoteQuality(notes)
    }
  };
}
```

**Context Usage in LLM Calls:**
```javascript
const prompt = `
You are extracting data from a ${context.pathology.primary} case.
This is a ${context.clinical.complexity} complexity case.

Focus on these pathology-specific fields:
${getPathologySpecificFields(context.pathology.primary)}

Here are patterns we've learned work well:
${context.learned.patterns.map(p => `- ${p.example}`).join('\n')}

Clinical note:
${notes}
`;
```

---

#### 6. Knowledge Base (Enhances LLM Prompts)

**File:** `src/services/knowledge/knowledgeBase.js`

**Role:** Store domain knowledge that guides LLM processing

**Knowledge Types:**
1. **Extraction Patterns** - Successful extraction examples
2. **Medical Terminology** - Neurosurgery-specific terms
3. **Abbreviation Dictionary** - Medical abbreviations
4. **Pathology Templates** - Expected data for each pathology
5. **Writing Styles** - Preferred narrative patterns

**Integration with LLM:**
```javascript
async function extractWithLLM(notes, context) {
  // Get relevant knowledge from base
  const knowledge = await knowledgeBase.getRelevantKnowledge({
    pathology: context.pathology.primary,
    task: 'extraction'
  });
  
  // Build enhanced prompt with knowledge
  const prompt = buildPrompt({
    notes,
    context,
    examples: knowledge.examples,
    terminology: knowledge.terminology,
    requirements: knowledge.requirements
  });
  
  // Call LLM with enhanced prompt
  return await callLLMWithFallback(prompt, {
    task: 'data_extraction',
    systemPrompt: knowledge.systemPrompt
  });
}
```

---

## Governance and Control Flow

### Who Controls the LLM?

LLMs in DCS are **governed by multiple layers** of control:

#### 1. User Controls (Highest Level)

**What Users Control:**
- âœ… **Model Selection**: Choose which LLM to use (Claude, GPT, Gemini)
- âœ… **Enable/Disable**: Turn LLM processing on or off
- âœ… **Force Patterns**: Use only pattern-based extraction
- âœ… **Review & Correct**: Override any LLM output
- âœ… **Learning**: Choose whether system learns from corrections

**UI Controls:**
- Settings â†’ Model Selection
- Settings â†’ Use LLM toggle
- Review tab â†’ Edit any field
- Learning Dashboard â†’ View/manage learned patterns

---

#### 2. Orchestration Layer (System Control)

**What Orchestrator Controls:**
- âœ… **When**: LLM only called at specific workflow points
- âœ… **How**: Determines parameters (temperature, tokens, etc.)
- âœ… **Quality Gates**: Validates outputs before proceeding
- âœ… **Iterations**: Limits refinement loops (max 2)
- âœ… **Fallback**: Switches to patterns if LLM fails

**Control Flow:**
```
User Request
    â†“
[Orchestrator Checks]
    â”œâ”€ Is LLM available? â†’ No â†’ Use Patterns
    â”œâ”€ Is LLM enabled? â†’ No â†’ Use Patterns
    â”œâ”€ Note complexity high? â†’ Yes â†’ Prefer LLM
    â””â”€ All checks pass â†’ Call LLM
         â†“
    [LLM Processes]
         â†“
    [Validation Checks]
         â”œâ”€ Quality too low? â†’ Re-extract or fail
         â”œâ”€ Missing required fields? â†’ Re-extract
         â””â”€ All good â†’ Proceed
              â†“
         [Next Phase]
```

---

#### 3. Validation Layer (Quality Control)

**What Validation Controls:**
- âœ… **Completeness**: All required fields extracted?
- âœ… **Accuracy**: Data makes medical sense?
- âœ… **Consistency**: No contradictions?
- âœ… **Confidence**: Extraction reliable enough?

**Validation Gates:**
```javascript
const validationGates = {
  // Gate 1: Required Fields
  requiredFields: ['pathology.primary', 'dates.admission', 'demographics.age'],
  
  // Gate 2: Quality Threshold
  minimumQuality: 0.7,  // 70% overall quality required
  
  // Gate 3: Confidence Threshold
  minimumConfidence: {
    critical: 0.8,  // Critical fields need 80%+ confidence
    normal: 0.5     // Normal fields need 50%+ confidence
  },
  
  // Gate 4: Logical Consistency
  logicRules: [
    'discharge_date > admission_date',
    'surgery_date >= admission_date',
    'age > 0 && age < 120'
  ]
};
```

If any gate fails â†’ LLM output rejected â†’ Re-extract or use patterns

---

#### 4. Cost and Performance Monitoring

**What Monitors Control:**
- âœ… **API Usage**: Track calls per provider
- âœ… **Cost Tracking**: Monitor spending in real-time
- âœ… **Performance**: Measure response times
- âœ… **Success Rate**: Track failures and fallbacks

**Automatic Controls:**
```javascript
const costControls = {
  // Automatic downgrade if costs too high
  monthlyBudget: 100.00,  // $100/month limit
  costPerCall: 0.05,      // $0.05 average per call
  
  // Automatic fallback if provider unavailable
  maxFailures: 3,         // After 3 failures, use different provider
  
  // Cache to reduce duplicate calls
  cacheEnabled: true,
  cacheDuration: 3600     // 1 hour cache
};
```

---

### Control Flow Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER CONTROLS                            â”‚
â”‚  â€¢ Model selection                                          â”‚
â”‚  â€¢ Enable/disable LLM                                       â”‚
â”‚  â€¢ Review and correct outputs                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ORCHESTRATION CONTROLS                         â”‚
â”‚  â€¢ Workflow decisions                                       â”‚
â”‚  â€¢ Quality thresholds                                       â”‚
â”‚  â€¢ Iteration limits                                         â”‚
â”‚  â€¢ Fallback logic                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 VALIDATION CONTROLS                          â”‚
â”‚  â€¢ Completeness checks                                      â”‚
â”‚  â€¢ Accuracy validation                                      â”‚
â”‚  â€¢ Consistency verification                                 â”‚
â”‚  â€¢ Confidence scoring                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MONITORING & COST CONTROLS                      â”‚
â”‚  â€¢ API usage tracking                                       â”‚
â”‚  â€¢ Cost monitoring                                          â”‚
â”‚  â€¢ Performance metrics                                      â”‚
â”‚  â€¢ Automatic fallback                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                    ğŸ¤– LLM API
                (Anthropic/OpenAI/Google)
```

**Key Principle**: LLMs are **tools controlled by the system**, not autonomous agents making independent decisions.

---

## LLM vs Pattern-Based Processing

### Comparison Table

| Aspect | Pattern-Based | LLM-Based | Hybrid (Recommended) |
|--------|---------------|-----------|---------------------|
| **Accuracy** | 85-90% | 92-98% | 95-98% |
| **Speed** | Very Fast (1-2s) | Medium (2-4s) | Medium (2-4s) |
| **Cost** | Free | $0.01-0.05/call | $0.01-0.05/call |
| **Reliability** | 100% available | 95-99% available | 99%+ (auto-fallback) |
| **Context Understanding** | Limited | Excellent | Excellent |
| **Abbreviation Handling** | Good (dictionary) | Excellent (contextual) | Excellent |
| **Writing Style Variation** | Struggles | Handles well | Handles well |
| **Required Setup** | None | API key needed | API key optional |
| **Privacy** | 100% local | Cloud processing | Cloud + local |
| **Learning Capability** | Rule-based only | Adapts naturally | Both approaches learn |

### When Each Method is Used

#### Pattern-Based Only
- **When:** LLM unavailable or disabled
- **Scenarios:**
  - No internet connection
  - No API key configured
  - User preference for local processing
  - API provider outage
  - Cost constraints

**Code:**
```javascript
const extraction = await extractMedicalEntities(notes, {
  useLLM: false,
  usePatterns: true
});
```

#### LLM-Based Only
- **When:** Patterns insufficient for complexity
- **Scenarios:**
  - Highly variable writing styles
  - Complex medical terminology
  - Subtle clinical relationships
  - Narrative generation (templates insufficient)

**Code:**
```javascript
const extraction = await extractMedicalEntities(notes, {
  useLLM: true,
  usePatterns: false
});
```

#### Hybrid (Default)
- **When:** Best of both worlds
- **Scenarios:**
  - Normal operation mode
  - Maximum accuracy desired
  - Validation and cross-checking needed

**Code:**
```javascript
const extraction = await extractMedicalEntities(notes, {
  useLLM: null,  // Auto-detect
  usePatterns: true  // Always run patterns as baseline
});
```

### Pattern Examples

**Pattern-Based Extraction:**
```javascript
// Regex patterns for specific data
const patterns = {
  age: /(?:age|yo|y\/o)\s*[:=]?\s*(\d{1,3})/i,
  mrn: /(?:mrn|mr#|medical record)\s*[:=#]?\s*(\d+)/i,
  gcs: /gcs\s*[:=]?\s*(\d{1,2})/i
};

// Limited context understanding
const ageMatch = text.match(patterns.age);
if (ageMatch) {
  extracted.demographics.age = ageMatch[1];
}
```

**LLM-Based Extraction:**
```javascript
// Rich context understanding
const prompt = `
From this clinical note, extract the patient's age.
Consider phrases like "62 year old", "62yo", "patient is 62", 
"62 years of age", and any other variations.

Note: ${text}
`;

const response = await callLLM(prompt);
// LLM understands context: "62 year old man" â†’ age: 62
```

---

## Error Handling and Fallback Mechanisms

### Fallback Hierarchy

```
Primary: User-Selected Model (e.g., Claude 3.5 Sonnet)
    â†“ [if fails]
Fallback 1: Best Alternative (e.g., GPT-4o)
    â†“ [if fails]
Fallback 2: Third Option (e.g., Gemini 1.5 Pro)
    â†“ [if fails]
Fallback 3: Fast Models (Haiku, GPT-4o Mini, Flash)
    â†“ [if all fail]
Final Fallback: Pattern-Based Extraction
```

### Error Types and Responses

#### 1. API Errors

**Error Types:**
- Authentication failure (invalid API key)
- Rate limiting (too many requests)
- Service unavailable (provider outage)
- Timeout (request took too long)

**Response:**
```javascript
try {
  return await callPrimaryLLM(prompt);
} catch (error) {
  if (error.code === 'auth_failed') {
    console.error('Invalid API key');
    // Try different provider
    return await callFallbackLLM(prompt);
  }
  
  if (error.code === 'rate_limit') {
    console.warn('Rate limited, retrying in 1s');
    await sleep(1000);
    return await callPrimaryLLM(prompt);
  }
  
  if (error.code === 'service_unavailable') {
    console.warn('Service down, using fallback');
    return await callFallbackLLM(prompt);
  }
  
  // If all LLMs fail, use patterns
  console.warn('All LLMs failed, using patterns');
  return await extractWithPatterns(notes);
}
```

#### 2. Quality Errors

**Error Types:**
- Low confidence extraction
- Missing required fields
- Inconsistent data
- Hallucinated information

**Response:**
```javascript
const validation = await validateExtraction(llmResult);

if (!validation.isValid) {
  // Log specific errors
  console.warn('LLM extraction issues:', validation.errors);
  
  // Try once more with improved prompt
  if (retries < 1) {
    return await extractWithLLM(notes, {
      feedback: validation.errors,  // Tell LLM what was wrong
      retries: retries + 1
    });
  }
  
  // If still fails, merge with pattern results
  const patternResult = await extractWithPatterns(notes);
  return mergeSafeResults(llmResult, patternResult, validation);
}
```

#### 3. Timeout Errors

**Response:**
```javascript
const controller = new AbortController();
const timeout = setTimeout(() => controller.abort(), 30000); // 30s timeout

try {
  const response = await fetch(llmApiUrl, {
    signal: controller.signal,
    // ... other options
  });
  clearTimeout(timeout);
  return response;
} catch (error) {
  if (error.name === 'AbortError') {
    console.warn('LLM call timed out, using faster model');
    return await callFastLLM(prompt); // Try Haiku or GPT-4o Mini
  }
  throw error;
}
```

### Automatic Recovery Strategies

#### 1. Retry with Exponential Backoff

```javascript
async function callLLMWithRetry(prompt, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await callLLM(prompt);
    } catch (error) {
      if (attempt === maxRetries) throw error;
      
      const delay = Math.pow(2, attempt) * 1000; // 2s, 4s, 8s
      console.log(`Retry ${attempt}/${maxRetries} in ${delay}ms`);
      await sleep(delay);
    }
  }
}
```

#### 2. Cache Fallback

```javascript
async function callLLMWithCache(prompt, options) {
  // Check cache first
  const cached = getCachedLLMResponse(prompt);
  if (cached && !options.bypassCache) {
    console.log('Using cached LLM response');
    return cached;
  }
  
  // Try LLM call
  try {
    const response = await callLLM(prompt);
    cacheLLMResponse(prompt, response);
    return response;
  } catch (error) {
    // If LLM fails but we have cache, use it even if old
    if (cached) {
      console.warn('LLM failed, using stale cache');
      return cached;
    }
    throw error;
  }
}
```

#### 3. Degraded Mode

```javascript
async function extractWithDegradedMode(notes) {
  console.warn('Operating in degraded mode (no LLM available)');
  
  return {
    extracted: await extractWithPatterns(notes),
    metadata: {
      mode: 'degraded',
      llmUnavailable: true,
      accuracy: 'reduced'
    },
    warning: 'LLM unavailable. Extraction accuracy may be reduced. Review carefully.'
  };
}
```

---

## Cost Tracking and Monitoring

### Cost Calculation

**Token Estimation:**
```javascript
function estimateTokens(text) {
  // Rough estimate: 1 token â‰ˆ 4 characters
  return Math.ceil(text.length / 4);
}
```

**Cost Calculation:**
```javascript
function calculateCost(inputTokens, outputTokens, model) {
  const inputCost = (inputTokens / 1000000) * model.costPer1MInput;
  const outputCost = (outputTokens / 1000000) * model.costPer1MOutput;
  return inputCost + outputCost;
}

// Example: Claude 3.5 Sonnet
// Input: 1000 tokens, Output: 500 tokens
// Cost: (1000/1M * $3) + (500/1M * $15) = $0.003 + $0.0075 = $0.0105
```

### Cost Tracking Storage

**LocalStorage Structure:**
```javascript
{
  "totalCost": 0.2847,
  "byProvider": {
    "Claude 3.5 Sonnet": {
      "cost": 0.1423,
      "calls": 8,
      "tokens": 45000
    },
    "GPT-4o": {
      "cost": 0.0982,
      "calls": 5,
      "tokens": 32000
    }
  },
  "byTask": {
    "data_extraction": {
      "cost": 0.1205,
      "calls": 7
    },
    "narrative_generation": {
      "cost": 0.1642,
      "calls": 6
    }
  },
  "history": [
    {
      "timestamp": "2025-10-19T10:30:00Z",
      "provider": "Claude 3.5 Sonnet",
      "task": "data_extraction",
      "inputTokens": 2843,
      "outputTokens": 1235,
      "cost": 0.0142,
      "duration": 2843,
      "success": true
    }
    // ... last 100 calls
  ]
}
```

### Real-Time Monitoring Dashboard

**UI Display:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’° LLM Usage & Cost Tracking                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Spent Today:        $0.28                             â”‚
â”‚ Total Calls Today:        16                                â”‚
â”‚ Average Cost/Call:        $0.0175                           â”‚
â”‚                                                             â”‚
â”‚ By Provider:                                                â”‚
â”‚ â”œâ”€ Claude 3.5 Sonnet      $0.14 (8 calls)  50%            â”‚
â”‚ â”œâ”€ GPT-4o                 $0.10 (5 calls)  34%            â”‚
â”‚ â””â”€ Gemini 1.5 Pro         $0.04 (3 calls)  16%            â”‚
â”‚                                                             â”‚
â”‚ By Task:                                                    â”‚
â”‚ â”œâ”€ Data Extraction        $0.12 (7 calls)  42%            â”‚
â”‚ â””â”€ Narrative Generation   $0.16 (6 calls)  58%            â”‚
â”‚                                                             â”‚
â”‚ Performance:                                                â”‚
â”‚ â”œâ”€ Success Rate           100%                             â”‚
â”‚ â”œâ”€ Average Duration       2.4s                             â”‚
â”‚ â””â”€ Cache Hit Rate         12%                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cost Optimization Features

#### 1. Response Caching

```javascript
const cacheKey = hashPrompt(prompt);
const cached = cache.get(cacheKey);

if (cached && (Date.now() - cached.timestamp < 3600000)) {
  console.log('âœ… Cache hit - saved $0.015');
  return cached.response;
}

// If not cached, call LLM and cache result
const response = await callLLM(prompt);
cache.set(cacheKey, {
  response,
  timestamp: Date.now(),
  cost: calculateCost(prompt, response)
});
```

**Savings:** ~$0.01-0.02 per cached response

#### 2. Prompt Optimization

```javascript
// Instead of sending entire note multiple times
const longNote = notes.join('\n'); // 10,000 tokens

// Extract once, reuse structured data
const extracted = await extractWithLLM(longNote); // $0.05

// Generate narrative from structured data (much smaller)
const narrative = await generateNarrative(extracted); // $0.01

// Total: $0.06 vs $0.10 if we sent full note twice
```

**Savings:** 30-40% cost reduction

#### 3. Model Selection

```javascript
// For simple extraction, use faster/cheaper model
if (complexity === 'simple') {
  model = 'claude-haiku'; // $0.25/$1.25 per 1M
} else {
  model = 'claude-sonnet-3.5'; // $3/$15 per 1M
}

// Haiku: ~$0.005 per call
// Sonnet: ~$0.015 per call
// Savings: $0.01 per simple case
```

**Savings:** 50-70% on simple cases

---

## Security and Privacy Considerations

### API Key Security

#### Backend Storage (Secure)

**File:** `backend/.env`
```env
ANTHROPIC_API_KEY=sk-ant-api03-...
OPENAI_API_KEY=sk-proj-...
GEMINI_API_KEY=AIzaSy...
```

**Security Features:**
- âœ… Never committed to Git (`.gitignore` includes `.env`)
- âœ… Never exposed to browser/frontend
- âœ… Only backend server has access
- âœ… Environment variables in production

#### Frontend (Never Stores Keys)

```javascript
// âŒ NEVER do this:
const apiKey = 'sk-ant-api03-...';  // Exposed in browser!

// âœ… Instead, call backend proxy:
const response = await fetch('http://localhost:3001/api/llm/anthropic', {
  method: 'POST',
  body: JSON.stringify({ prompt, messages })
  // No API key sent from frontend
});
```

### PHI Protection

#### Data Sent to LLMs

**What IS sent:**
- âœ… Clinical note text (necessary for extraction)
- âœ… Structured medical data (for narrative generation)
- âœ… Medical context (pathology, specialty)

**What is NOT sent:**
- âŒ Patient names
- âŒ Medical Record Numbers (MRN)
- âŒ Specific dates (converted to relative times)
- âŒ Phone numbers, addresses
- âŒ Other direct identifiers

**Anonymization Process:**
```javascript
function anonymizeForLLM(clinicalNote) {
  let anonymized = clinicalNote;
  
  // Replace names with generic identifiers
  anonymized = anonymized.replace(/\b[A-Z][a-z]+ [A-Z][a-z]+\b/g, '[PATIENT]');
  
  // Replace MRNs
  anonymized = anonymized.replace(/\b\d{6,10}\b/g, '[MRN]');
  
  // Convert absolute dates to relative
  anonymized = convertDatesToRelative(anonymized);
  
  return anonymized;
}
```

**Note:** Full PHI anonymization is currently NOT implemented for LLM calls. This is a planned security enhancement. Currently, the system relies on:
1. HIPAA Business Associate Agreements (BAAs) with LLM providers
2. Temporary processing (no PHI stored by providers per their policies)
3. User awareness and consent

### Data Storage

#### Local Storage (Browser)

**What is stored:**
- âœ… Anonymized learning patterns
- âœ… Cost tracking data
- âœ… Performance metrics
- âœ… User preferences

**What is NOT stored:**
- âŒ API keys (only in backend)
- âŒ Complete clinical notes (unless user explicitly saves draft)
- âŒ Patient identifiers

#### LLM Provider Storage

**Provider Policies:**
- **Anthropic**: No training on customer data, 30-day retention for abuse monitoring
- **OpenAI**: Optional zero-retention via API settings
- **Google**: No training on API data, temporary processing only

**DCS Configuration:**
- Requests zero-retention where available
- Uses enterprise API endpoints
- Logs provider responses for debugging (local only)

---

## Code Examples

### Example 1: Basic LLM Call

```javascript
import { callLLMWithFallback } from './services/llmService.js';

// Simple extraction call
const prompt = `
Extract the patient's age from this note:
"62 year old male presented with sudden severe headache..."
`;

const response = await callLLMWithFallback(prompt, {
  task: 'simple_extraction',
  maxTokens: 100,
  temperature: 0.1  // Low temperature for factual extraction
});

console.log(response); // "62"
```

### Example 2: Extraction with Context

```javascript
import { extractMedicalEntities } from './services/extraction.js';

const notes = `
POD 3: Patient continues to improve. 
GCS 15, following commands. 
Drain output 20cc. No signs of vasospasm.
`;

const extraction = await extractMedicalEntities(notes, {
  useLLM: true,
  context: {
    pathology: 'subarachnoid_hemorrhage',
    pod: 3
  }
});

console.log(extraction);
/*
{
  dates: { pod: 3 },
  neurologicalExam: { gcs: 15, command_following: true },
  procedures: { drain: { output: '20cc' } },
  complications: { vasospasm: false }
}
*/
```

### Example 3: Narrative Generation

```javascript
import { generateNarrative } from './services/narrativeEngine.js';

const extractedData = {
  demographics: { age: 62, sex: 'male' },
  pathology: { primary: 'subarachnoid_hemorrhage', location: 'anterior communicating artery' },
  dates: { admission: '2024-10-01', surgery: '2024-10-02', discharge: '2024-10-10' },
  procedures: [
    { name: 'aneurysm coiling', date: '2024-10-02', outcome: 'successful' }
  ]
};

const narrative = await generateNarrative(extractedData, {
  useLLM: true,
  style: 'formal',
  pathologyType: 'subarachnoid_hemorrhage'
});

console.log(narrative.hospitalCourse);
/*
"The patient is a 62-year-old male who presented with sudden severe headache 
and was found to have subarachnoid hemorrhage from an anterior communicating 
artery aneurysm. He underwent successful endovascular coiling on 2024-10-02..."
*/
```

### Example 4: Orchestrated Summary Generation

```javascript
import { orchestrateSummaryGeneration } from './services/summaryOrchestrator.js';

const notes = [
  'POD 0: Patient underwent successful aneurysm coiling...',
  'POD 1: Remains in ICU. GCS 15. No complications...',
  'POD 2: Transferred to step-down. Ambulating with PT...'
];

const result = await orchestrateSummaryGeneration(notes, {
  enableLearning: true,
  enableFeedbackLoops: true,
  maxRefinementIterations: 2,
  qualityThreshold: 0.8
});

console.log(result);
/*
{
  success: true,
  extractedData: { ... structured data ... },
  validation: { isValid: true, quality: 0.87 },
  intelligence: { timeline: [...], treatmentResponse: [...] },
  summary: { 
    chiefComplaint: '...',
    hospitalCourse: '...',
    dischargePlan: '...'
  },
  refinementIterations: 1,
  metadata: {
    processingTime: 20733,
    llmProvider: 'Claude 3.5 Sonnet',
    llmCost: 0.0234
  }
}
*/
```

### Example 5: Error Handling

```javascript
import { extractMedicalEntities } from './services/extraction.js';

async function safeExtraction(notes) {
  try {
    // Try LLM first
    return await extractMedicalEntities(notes, { useLLM: true });
  } catch (error) {
    console.error('LLM extraction failed:', error);
    
    if (error.code === 'auth_failed') {
      // Invalid API key - use patterns only
      return await extractMedicalEntities(notes, { 
        useLLM: false,
        usePatterns: true 
      });
    }
    
    if (error.code === 'rate_limit') {
      // Rate limited - wait and retry
      await new Promise(resolve => setTimeout(resolve, 5000));
      return await extractMedicalEntities(notes, { useLLM: true });
    }
    
    // Unknown error - fallback to patterns
    return await extractMedicalEntities(notes, { 
      useLLM: false,
      usePatterns: true 
    });
  }
}
```

### Example 6: Cost Monitoring

```javascript
import { getCostTracking, resetCostTracking } from './services/llmService.js';

// Get current costs
const costs = getCostTracking();

console.log(`Total spent: $${costs.totalCost.toFixed(4)}`);
console.log(`Claude calls: ${costs.byProvider['Claude 3.5 Sonnet'].calls}`);
console.log(`Extraction cost: $${costs.byTask.data_extraction.cost.toFixed(4)}`);

// Check if over budget
const MONTHLY_BUDGET = 100.00;
if (costs.totalCost > MONTHLY_BUDGET * 0.8) {
  console.warn('âš ï¸ 80% of monthly budget used. Consider using cheaper models.');
}

// Reset at end of month
if (new Date().getDate() === 1) {
  resetCostTracking();
  console.log('âœ… Monthly cost tracking reset');
}
```

---

## FAQ

### Q: Can DCS work without LLMs?

**A: Yes, absolutely!** DCS has a complete pattern-based extraction system that works without any LLMs. You will get 85-90% accuracy instead of 92-98%, but the system is fully functional.

### Q: Which LLM provider is best?

**A: Claude 3.5 Sonnet (Anthropic) is recommended** for medical text based on testing. However, all three providers (Anthropic, OpenAI, Google) work well. Choose based on:
- Cost preferences
- API availability in your region
- Personal experience
- Specific feature needs

### Q: How much does LLM usage cost?

**A: Typical costs:**
- Per extraction: $0.01-0.02
- Per narrative: $0.02-0.03
- Per complete summary: $0.03-0.05
- Monthly (moderate use): $10-25

Use the cost tracking dashboard to monitor your actual spending.

### Q: Are LLMs making medical decisions?

**A: No.** LLMs are extraction and writing tools only. They:
- âœ… Extract facts from notes
- âœ… Generate narrative text
- âŒ Never diagnose
- âŒ Never recommend treatments
- âŒ Never make clinical decisions

All medical decisions remain with the healthcare provider.

### Q: What happens if the LLM hallucinates?

**A: Multiple safeguards prevent this:**
1. Validation layer checks all outputs
2. Pattern-based extraction provides cross-validation
3. Confidence scores flag uncertain data
4. User review step catches errors
5. Learning system improves over time

Invalid/hallucinated data is rejected before reaching the user.

### Q: Can I use my own LLM/custom model?

**A: Not currently.** The system supports only Anthropic, OpenAI, and Google APIs. Custom model support could be added but would require:
- API compatibility layer
- Testing for medical accuracy
- Cost calculation updates
- Documentation updates

### Q: Is patient data sent to LLM providers?

**A: Yes, clinical note text is sent** for processing. However:
- Providers have HIPAA Business Associate Agreements
- Most providers don't store API data long-term
- Zero-retention options used where available
- Full anonymization is planned for future versions

See [Security and Privacy](#security-and-privacy-considerations) section for details.

### Q: How do I reduce LLM costs?

**A: Several strategies:**
1. Use caching for similar extractions
2. Select cheaper models for simple cases (Haiku, GPT-4o Mini)
3. Use pattern-only mode when acceptable
4. Batch process multiple notes
5. Enable learning to improve pattern accuracy over time

### Q: What if multiple LLM providers fail?

**A: The system automatically falls back** to pattern-based extraction. You'll see a warning that accuracy may be reduced, but the system continues working.

### Q: Can I train the LLM on my institution's data?

**A: Not directly,** but the learning system achieves similar results:
- User corrections teach the system
- Imported summaries teach composition
- Learned patterns apply to future extractions
- All learning is local (no cloud training)

This provides custom adaptation without needing to fine-tune the LLM.

### Q: How is LLM different from the "ML Learning System"?

**A: Different components:**

**LLM (Cloud-based AI):**
- External service (Anthropic/OpenAI/Google)
- General language understanding
- Extraction and generation
- Requires API key and internet

**ML Learning System (Local):**
- Local browser-based system
- Learns from your corrections
- Stores patterns and preferences
- Works offline
- No API needed

They work together: LLM provides intelligence, ML learning makes it adapt to your needs.

---

## Conclusion

LLMs in DCS serve as **intelligent processing engines** that operate within a **governed, validated, and monitored framework**. They are:

- **Controlled by**: Multi-layer orchestration and validation
- **Enhanced by**: Pattern-based extraction and medical knowledge
- **Monitored by**: Cost tracking and performance metrics
- **Validated by**: Comprehensive quality checks
- **Improved by**: Local ML learning system

The system is designed to work **with or without** LLMs, ensuring reliability and flexibility while maximizing accuracy when LLMs are available.

---

**For More Information:**
- [README.md](./README.md) - System overview and setup
- [ARCHITECTURE.md](./ARCHITECTURE.md) - Complete system architecture
- [ENHANCED_LLM_SYSTEM.md](./ENHANCED_LLM_SYSTEM.md) - LLM feature implementation details
- [SECURITY.md](./SECURITY.md) - Security and privacy documentation

**Questions or Issues:**
- [GitHub Issues](https://github.com/ramihatou97/DCS/issues)
- See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for common problems

---

**Document Version:** 1.0  
**Maintained By:** DCS Development Team  
**Next Review:** January 2026
