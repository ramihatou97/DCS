# LLM Roles and Architecture

**Version:** 2.0  
**Last Updated:** October 2025  
**Status:** Production-Ready  

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [LLM as AI Context-Aware Engine](#llm-as-ai-context-aware-engine)
3. [Governing Pattern-Based Retraction](#governing-pattern-based-retraction)
4. [Influencing Template-Based Generation](#influencing-template-based-generation)
5. [Governing Quality and Accuracy Assessment](#governing-quality-and-accuracy-assessment)
6. [Generating High-Quality Discharge Summaries](#generating-high-quality-discharge-summaries)
7. [Multi-Provider Architecture](#multi-provider-architecture)
8. [Intelligent Feedback Loops](#intelligent-feedback-loops)
9. [Quality Governance Framework](#quality-governance-framework)
10. [Performance Metrics and Monitoring](#performance-metrics-and-monitoring)

---

## Executive Summary

The **LLM (Large Language Model) System** serves as the **AI Context-Aware Engine** at the heart of the Discharge Summary Generator (DCS). Rather than operating as a rigid, rule-based system, the LLM dynamically governs and enhances all aspects of medical document generation through intelligent, context-sensitive decision-making.

### Core Principles

1. **Flexibility Over Rigidity**: The LLM system uses adaptive pattern-based retraction that evolves based on context, clinical scenarios, and learned patterns
2. **Template Enhancement**: Templates serve as structural guides that the LLM intelligently enhances and adapts based on patient-specific context
3. **Quality Governance**: The LLM actively participates in quality assessment, identifying gaps, inconsistencies, and areas for improvement
4. **Continuous Learning**: The system learns from corrections, imported summaries, and clinical feedback to continuously improve

### LLM Responsibilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LLM AS AI CONTEXT-AWARE ENGINE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   PATTERN-BASED   â”‚     â”‚  TEMPLATE-BASED   â”‚              â”‚
â”‚  â”‚    RETRACTION     â”‚â”€â”€â”€â”€â–¶â”‚    GENERATION     â”‚              â”‚
â”‚  â”‚   (Flexible)      â”‚     â”‚   (Enhanced)      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                         â”‚                          â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â”‚                                         â”‚
â”‚                      â–¼                                         â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚         â”‚  QUALITY & ACCURACY     â”‚                           â”‚
â”‚         â”‚     GOVERNANCE          â”‚                           â”‚
â”‚         â”‚  (Continuous Assessment)â”‚                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                    â”‚                                           â”‚
â”‚                    â–¼                                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚         â”‚   HIGH-QUALITY          â”‚                           â”‚
â”‚         â”‚  DISCHARGE SUMMARIES    â”‚                           â”‚
â”‚         â”‚   (Clinical Excellence) â”‚                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## LLM as AI Context-Aware Engine

### Context Awareness Architecture

The LLM operates as an **intelligent orchestration layer** that maintains awareness of:

1. **Clinical Context**
   - Patient pathology type and severity
   - Disease progression and timeline
   - Treatment interventions and responses
   - Functional status evolution
   - Comorbidities and complications

2. **Temporal Context**
   - Admission to discharge timeline
   - Pre-operative vs post-operative periods
   - Acute vs chronic phases
   - Event sequences and causal relationships

3. **Knowledge Context**
   - Learned patterns from previous cases
   - Domain-specific neurosurgical knowledge
   - Institutional preferences and style
   - Medical writing standards

4. **Quality Context**
   - Completeness requirements
   - Accuracy standards
   - Consistency expectations
   - Narrative quality benchmarks

### Context Provider Integration

**File:** `src/services/context/contextProvider.js`

The LLM interfaces with the Context Provider to access:

```javascript
{
  clinicalContext: {
    pathology: { type, subtype, severity },
    timeline: { admission, procedures, discharge },
    treatments: [{ intervention, timing, response }],
    functionalStatus: { initial, progression, final }
  },
  
  knowledgeContext: {
    learnedPatterns: [...],
    domainKnowledge: {...},
    stylePreferences: {...}
  },
  
  qualityContext: {
    thresholds: { completeness: 0.85, accuracy: 0.90 },
    previousIterations: [...],
    identifiedGaps: [...]
  }
}
```

### Adaptive Processing

The LLM adapts its processing based on context:

- **Simple Cases**: Uses efficient processing with standard templates
- **Complex Cases**: Applies deep reasoning and enhanced pattern matching
- **Ambiguous Data**: Seeks clarification and applies conservative interpretation
- **Missing Data**: Intelligently identifies gaps without fabrication

---

## Governing Pattern-Based Retraction

### Flexible Pattern System (Not Rigid)

Traditional rule-based systems fail because clinical documentation is inherently variable and context-dependent. The LLM-governed pattern system is **adaptive and flexible**.

### Pattern Categories

#### 1. Static Patterns (Baseline)
**Location:** `src/config/pathologyPatterns.js`

```javascript
// Example: Rigid pattern (what we avoid)
const RIGID_PATTERN = /blood pressure: (\d+)\/(\d+)/i;

// Example: Flexible pattern (what LLM uses)
const FLEXIBLE_BP_EXTRACTION = {
  patterns: [
    /bp:?\s*(\d+)\/(\d+)/i,
    /blood pressure.*?(\d+)\/(\d+)/i,
    /pressure.*?(\d+)\s*over\s*(\d+)/i,
    /(\d+)\/(\d+)\s*mmHg/i
  ],
  llmEnhancement: true,  // LLM validates and contextualizes
  contextRules: {
    checkPlausibility: true,
    validateRange: { systolic: [60, 250], diastolic: [30, 180] },
    considerTrend: true,
    identifyAbnormal: true
  }
}
```

#### 2. Dynamic Pattern Enhancement

The LLM **enhances** static patterns by:

1. **Context Validation**
   - Verifies extracted data makes clinical sense
   - Identifies potential extraction errors
   - Flags implausible values

2. **Semantic Understanding**
   - Understands synonyms and variations
   - Handles abbreviations contextually
   - Interprets complex medical terminology

3. **Relationship Detection**
   - Links related information across notes
   - Identifies cause-effect relationships
   - Tracks temporal sequences

4. **Negation Handling**
   - Detects negated findings ("no evidence of...")
   - Handles uncertainty ("possible", "questionable")
   - Distinguishes historical vs current

### Adaptive Retraction Process

**File:** `src/services/extraction.js`

```javascript
/**
 * LLM-Enhanced Pattern-Based Extraction
 * 
 * The LLM governs the entire extraction process, making it flexible
 * and context-aware rather than rigid and rule-based
 */
async function llmGovernedExtraction(notes, context) {
  // Step 1: Initial pattern matching (baseline)
  const baselineExtraction = applyStaticPatterns(notes);
  
  // Step 2: LLM validation and enhancement
  const enhancedExtraction = await llm.enhance({
    baseline: baselineExtraction,
    context: context,
    task: 'validate_and_enhance_extraction',
    instructions: `
      Review the pattern-based extraction and:
      1. Validate clinical plausibility
      2. Identify missed information
      3. Resolve ambiguities using context
      4. Flag potential errors
      5. Enhance with semantic understanding
      
      DO NOT fabricate data. Only enhance what's documented.
    `
  });
  
  // Step 3: Confidence scoring and selective retraction
  const validated = await llm.validateWithConfidence({
    enhanced: enhancedExtraction,
    context: context,
    retractIfUncertain: true,  // Retract low-confidence data
    minConfidence: 0.70
  });
  
  return validated;
}
```

### Pattern Learning and Evolution

The LLM system continuously learns and evolves patterns:

#### User Correction Learning
**File:** `src/services/ml/learningEngine.js`

```javascript
/**
 * When user corrects extraction, LLM learns the pattern
 */
async function learnFromCorrection(original, corrected, context) {
  // LLM analyzes the correction
  const pattern = await llm.analyzeCorrection({
    original: anonymize(original),
    corrected: anonymize(corrected),
    context: anonymize(context),
    task: 'pattern_learning',
    instructions: `
      Analyze this correction and extract:
      1. What pattern was missed?
      2. What context clues indicate the correct value?
      3. How can future extractions be improved?
      4. What are the boundary conditions?
      
      Generate a reusable pattern that applies to similar cases.
    `
  });
  
  // Store learned pattern with confidence
  await knowledgeBase.storePattern({
    pattern: pattern,
    confidence: 0.60,  // Initial confidence
    source: 'user_correction',
    verified: false,
    usageCount: 0
  });
}
```

#### Pattern Confidence Evolution

Patterns gain confidence through validation:

```javascript
/**
 * Pattern confidence increases with successful usage
 */
function updatePatternConfidence(patternId, successful) {
  const pattern = knowledgeBase.getPattern(patternId);
  
  pattern.usageCount++;
  
  if (successful) {
    // Increase confidence (max 0.95)
    pattern.confidence = Math.min(0.95, 
      pattern.confidence + (1 - pattern.confidence) * 0.1
    );
    pattern.successCount++;
  } else {
    // Decrease confidence (min 0.20)
    pattern.confidence = Math.max(0.20,
      pattern.confidence * 0.85
    );
  }
  
  // Retire low-performing patterns
  if (pattern.confidence < 0.30 && pattern.usageCount > 10) {
    pattern.status = 'retired';
  }
  
  knowledgeBase.updatePattern(patternId, pattern);
}
```

### Flexible vs Rigid Comparison

| Aspect | Rigid System (âŒ) | LLM-Governed System (âœ…) |
|--------|------------------|------------------------|
| **Pattern Matching** | Exact regex matches only | Semantic understanding + patterns |
| **Context** | Ignores context | Context-aware validation |
| **Ambiguity** | Fails on variations | Resolves using LLM reasoning |
| **Learning** | Static rules | Continuous learning from corrections |
| **Negation** | Misses negated findings | Understands negation and uncertainty |
| **Errors** | Accepts invalid data | Validates clinical plausibility |
| **Evolution** | Manual updates required | Automatic pattern evolution |

---

## Influencing Template-Based Generation

### Templates as Guidance, Not Constraints

The LLM treats templates as **structural guidance** that it intelligently **enhances and adapts** based on patient-specific context.

### Template Architecture

**File:** `src/utils/templates.js`

```javascript
/**
 * Pathology-specific templates provide structure
 * LLM provides intelligence and adaptation
 */
const GLIOBLASTOMA_TEMPLATE = {
  structure: {
    sections: [
      'chiefComplaint',
      'historyOfPresentIllness',
      'hospitalCourse',
      'procedures',
      'pathologyFindings',
      'complications',
      'dischargeStatus',
      'dischargePlan'
    ]
  },
  
  guidelines: {
    chiefComplaint: {
      target: '1-2 sentences',
      mustInclude: ['age', 'presenting symptom', 'diagnosis'],
      style: 'concise, direct',
      llmInstructions: 'Adapt length and detail based on complexity'
    },
    
    hospitalCourse: {
      target: 'chronological narrative',
      mustInclude: ['surgery', 'recovery', 'complications', 'treatment'],
      style: 'professional medical writing',
      llmInstructions: `
        Generate chronological narrative that:
        1. Follows logical temporal sequence
        2. Highlights clinically significant events
        3. Explains treatment decisions with context
        4. Connects cause and effect
        5. Maintains professional medical tone
        
        Adapt detail level based on:
        - Case complexity
        - Number of events
        - Clinical significance
        - Available information
      `
    }
  },
  
  pathologySpecific: {
    emphasize: [
      'tumor characteristics (size, location, enhancement)',
      'surgical approach and extent of resection',
      'pathology confirmation',
      'adjuvant therapy plans',
      'functional outcomes'
    ],
    
    followUpInstructions: {
      immediate: '2 weeks - wound check, staple removal',
      shortTerm: '4-6 weeks - start adjuvant therapy',
      longTerm: 'Ongoing - oncology follow-up, surveillance MRI'
    }
  }
};
```

### LLM Template Enhancement Process

**File:** `src/services/narrativeEngine.js`

```javascript
/**
 * LLM-Enhanced Narrative Generation
 * 
 * Templates provide structure, LLM provides intelligence
 */
async function generateLLMEnhancedNarrative(extractedData, template) {
  // Get base template for pathology
  const baseTemplate = getTemplateByPathology(extractedData.pathology.type);
  
  // LLM analyzes data and determines adaptation strategy
  const adaptationStrategy = await llm.planNarrative({
    data: extractedData,
    template: baseTemplate,
    task: 'narrative_planning',
    instructions: `
      Analyze the extracted data and template, then:
      
      1. ASSESS CASE COMPLEXITY:
         - Simple: Standard presentation, straightforward course
         - Moderate: Some complications or unusual features
         - Complex: Multiple complications, unusual presentation
      
      2. DETERMINE ADAPTATION NEEDS:
         - Should any sections be expanded? Why?
         - Should any sections be condensed? Why?
         - Are there unique aspects requiring special emphasis?
      
      3. IDENTIFY NARRATIVE PRIORITIES:
         - What are the most clinically significant events?
         - What relationships need to be highlighted?
         - What context is essential for understanding?
      
      4. PLAN ENHANCEMENTS:
         - What template sections need customization?
         - What additional context should be added?
         - How should temporal flow be structured?
      
      Return a structured plan for narrative generation.
    `
  });
  
  // Generate each section with LLM intelligence
  const narrative = {};
  
  for (const section of baseTemplate.structure.sections) {
    const sectionGuidelines = baseTemplate.guidelines[section];
    const sectionData = extractRelevantData(extractedData, section);
    
    narrative[section] = await llm.generateSection({
      section: section,
      data: sectionData,
      guidelines: sectionGuidelines,
      adaptationStrategy: adaptationStrategy[section],
      context: {
        previousSections: narrative,
        overallNarrative: adaptationStrategy.overallPlan,
        pathology: extractedData.pathology
      },
      task: `narrative_section_${section}`,
      instructions: `
        Generate the ${section} section following these principles:
        
        STRUCTURAL GUIDANCE (from template):
        ${JSON.stringify(sectionGuidelines, null, 2)}
        
        ADAPTATION STRATEGY:
        ${JSON.stringify(adaptationStrategy[section], null, 2)}
        
        KEY PRINCIPLES:
        1. Follow template structure but adapt based on context
        2. Use professional medical writing style
        3. Maintain chronological coherence
        4. Prioritize clinical significance
        5. Be concise but complete
        6. Use appropriate medical terminology
        7. Connect to previous sections naturally
        
        STRICT RULES:
        - Never fabricate information
        - Only include documented findings
        - Mark uncertain information appropriately
        - Maintain temporal accuracy
        - Use past tense for completed events
      `
    });
  }
  
  return narrative;
}
```

### Dynamic Template Adaptation Examples

#### Example 1: Simple Case

```javascript
// Template suggests: "Standard hospital course section"
// LLM adapts: Concise, focused narrative

Template Guidance: "Describe hospital course"
LLM Enhancement: 
  "The patient underwent uncomplicated craniotomy for tumor resection. 
   Post-operative recovery was unremarkable. Patient was ambulating 
   independently by POD 2 and discharged home on POD 4."
```

#### Example 2: Complex Case

```javascript
// Template suggests: "Standard hospital course section"
// LLM adapts: Detailed, chronological narrative with complications

Template Guidance: "Describe hospital course"
LLM Enhancement:
  "The patient underwent craniotomy for tumor resection on HD 1. 
   Post-operative course was complicated by development of symptomatic 
   cerebral edema on POD 2, requiring ICU transfer and management with 
   hyperosmolar therapy. Repeat imaging demonstrated resolution of edema 
   by POD 5. Patient was subsequently transferred to floor where 
   rehabilitation progressed steadily. By POD 8, patient demonstrated 
   significant functional improvement and was deemed safe for discharge 
   with outpatient therapy."
```

### Learned Template Patterns

The LLM learns institutional and provider preferences:

**File:** `src/services/ml/learningEngine.js`

```javascript
/**
 * Learn template preferences from imported summaries
 */
async function learnTemplatePreferences(importedSummary) {
  const patterns = await llm.analyzeStyle({
    summary: anonymize(importedSummary),
    task: 'style_learning',
    instructions: `
      Analyze this discharge summary and extract:
      
      1. STRUCTURAL PATTERNS:
         - Section organization
         - Section lengths
         - Detail level per section
      
      2. WRITING STYLE:
         - Sentence structure preferences
         - Terminology usage
         - Abbreviation handling
         - Tense usage
      
      3. EMPHASIS PATTERNS:
         - What information is prioritized?
         - What details are consistently included?
         - What is typically omitted?
      
      4. FOLLOW-UP FORMATTING:
         - How are appointments structured?
         - Medication reconciliation format
         - Instruction detail level
      
      Extract reusable patterns for future generation.
    `
  });
  
  // Store learned patterns
  await knowledgeBase.storeStylePatterns({
    structural: patterns.structural,
    stylistic: patterns.stylistic,
    emphasis: patterns.emphasis,
    formatting: patterns.formatting,
    confidence: 0.70,
    source: 'imported_summary'
  });
}
```

---

## Governing Quality and Accuracy Assessment

### LLM as Quality Governor

The LLM actively participates in quality assessment throughout the generation process, not just at the end.

### 6-Dimension Quality System

**File:** `src/services/qualityMetrics.js`

The LLM evaluates quality across 6 dimensions:

1. **Completeness (30%)** - Are all required sections and data present?
2. **Accuracy (25%)** - Does the data match source notes?
3. **Consistency (20%)** - Are dates, facts, and narratives aligned?
4. **Narrative Quality (15%)** - Is the writing professional and clear?
5. **Specificity (5%)** - Are details precise vs vague?
6. **Timeliness (5%)** - Is the generation efficient and data fresh?

### LLM Quality Assessment Integration

```javascript
/**
 * LLM-Enhanced Quality Assessment
 * 
 * The LLM doesn't just score - it explains and guides improvement
 */
async function llmGovernedQualityAssessment(extractedData, narrative, sourceNotes) {
  // Calculate baseline metrics
  const baselineMetrics = calculateQualityMetrics(
    extractedData, 
    narrative, 
    sourceNotes
  );
  
  // LLM deep analysis
  const llmAnalysis = await llm.analyzeQuality({
    extracted: extractedData,
    narrative: narrative,
    source: sourceNotes,
    metrics: baselineMetrics,
    task: 'quality_governance',
    instructions: `
      Perform comprehensive quality assessment:
      
      1. COMPLETENESS REVIEW:
         - What required information is missing?
         - What sections need more detail?
         - What data gaps are critical vs minor?
      
      2. ACCURACY VERIFICATION:
         - Does extracted data match source notes?
         - Are there any potential extraction errors?
         - Are dates and values plausible?
         - Identify any potential hallucinations
      
      3. CONSISTENCY CHECK:
         - Are dates logically sequenced?
         - Do narrative and structured data align?
         - Are references cross-consistent?
         - Do functional scores show logical progression?
      
      4. NARRATIVE QUALITY:
         - Is medical writing professional?
         - Is chronology clear and logical?
         - Are transitions smooth?
         - Is terminology appropriate?
         - Is the tone consistent?
      
      5. SPECIFICITY ASSESSMENT:
         - Are measurements precise?
         - Are vague terms used where specific data exists?
         - Is location information detailed enough?
      
      6. OVERALL QUALITY:
         - Rate overall quality (0-1 scale)
         - Identify top 3 improvement priorities
         - Suggest specific enhancements
      
      For each dimension:
      - Provide detailed score with justification
      - List specific issues found
      - Suggest concrete improvements
      - Rate severity (critical/major/minor/warning)
    `
  });
  
  // Synthesize final quality assessment
  return {
    overall: {
      score: llmAnalysis.overallScore,
      rating: getQualityRating(llmAnalysis.overallScore),
      confidence: llmAnalysis.confidence
    },
    
    dimensions: llmAnalysis.dimensions,
    
    issues: llmAnalysis.issues,
    
    recommendations: llmAnalysis.recommendations,
    
    actionable: {
      criticalFixes: llmAnalysis.criticalFixes,  // Must fix
      improvements: llmAnalysis.improvements,     // Should fix
      enhancements: llmAnalysis.enhancements      // Nice to have
    },
    
    metadata: {
      assessmentMethod: 'llm_governed',
      baselineMetrics: baselineMetrics,
      llmProvider: llmAnalysis.provider,
      timestamp: new Date().toISOString()
    }
  };
}
```

### Continuous Quality Monitoring

The LLM monitors quality at every phase:

```javascript
/**
 * Phase-by-Phase Quality Governance
 */
const QUALITY_CHECKPOINTS = {
  
  afterExtraction: async (extractedData, sourceNotes) => {
    const assessment = await llm.quickQualityCheck({
      data: extractedData,
      source: sourceNotes,
      focus: 'completeness_and_accuracy',
      instructions: `
        Quick extraction quality check:
        1. Are required fields extracted?
        2. Do values seem plausible?
        3. Any obvious errors or omissions?
        
        Return: { pass: boolean, issues: [], confidence: number }
      `
    });
    
    if (!assessment.pass && assessment.confidence > 0.80) {
      console.warn('[Quality] Extraction issues detected:', assessment.issues);
      return { shouldRefine: true, issues: assessment.issues };
    }
    
    return { shouldRefine: false };
  },
  
  afterIntelligence: async (intelligence, extractedData) => {
    const assessment = await llm.quickQualityCheck({
      intelligence: intelligence,
      data: extractedData,
      focus: 'consistency_and_logic',
      instructions: `
        Timeline and intelligence quality check:
        1. Is timeline logically sequenced?
        2. Are treatment-outcome pairs valid?
        3. Do functional scores show logical progression?
        4. Any temporal inconsistencies?
        
        Return: { pass: boolean, issues: [], confidence: number }
      `
    });
    
    if (!assessment.pass && assessment.confidence > 0.80) {
      console.warn('[Quality] Intelligence issues detected:', assessment.issues);
      return { shouldRefine: true, issues: assessment.issues };
    }
    
    return { shouldRefine: false };
  },
  
  afterNarrative: async (narrative, extractedData, sourceNotes) => {
    // Full quality assessment (already shown above)
    return await llmGovernedQualityAssessment(
      extractedData,
      narrative,
      sourceNotes
    );
  }
};
```

### Quality-Driven Refinement

The LLM uses quality assessment to guide refinement:

**File:** `src/services/summaryOrchestrator.js`

```javascript
/**
 * Intelligent refinement based on quality assessment
 */
async function intelligentRefinement(currentResult, qualityAssessment) {
  if (qualityAssessment.overall.score >= 0.85) {
    console.log('[Orchestrator] Quality threshold met, proceeding');
    return { shouldRefine: false };
  }
  
  // LLM determines refinement strategy
  const strategy = await llm.planRefinement({
    current: currentResult,
    quality: qualityAssessment,
    task: 'refinement_planning',
    instructions: `
      Quality score is ${qualityAssessment.overall.score}.
      Target is 0.85 or higher.
      
      Critical Issues:
      ${JSON.stringify(qualityAssessment.actionable.criticalFixes, null, 2)}
      
      Plan a refinement strategy:
      
      1. PRIORITIZE FIXES:
         - Which issues have highest impact?
         - Which are quickest to fix?
         - Which are blocking quality threshold?
      
      2. REFINEMENT APPROACH:
         - Re-extraction needed? (if accuracy issues)
         - Re-generation needed? (if narrative issues)
         - Enhancement only? (if minor issues)
      
      3. SPECIFIC ACTIONS:
         - What specific data should be re-extracted?
         - What sections should be re-generated?
         - What can be fixed with targeted edits?
      
      4. EXPECTED IMPROVEMENT:
         - Estimate quality score after fixes
         - Identify remaining known limitations
      
      Return a structured refinement plan.
    `
  });
  
  console.log('[Orchestrator] Refinement strategy:', strategy);
  
  return {
    shouldRefine: true,
    strategy: strategy,
    targetImprovements: strategy.targetImprovements,
    estimatedNewScore: strategy.estimatedScore
  };
}
```

---

## Generating High-Quality Discharge Summaries

### Quality as Primary Objective

The LLM system is designed with **quality as the primary objective**, not speed or cost.

### High-Quality Generation Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          HIGH-QUALITY SUMMARY GENERATION               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  Phase 1: INTELLIGENT EXTRACTION                      â”‚
â”‚  â”œâ”€ LLM-enhanced pattern matching                     â”‚
â”‚  â”œâ”€ Context-aware validation                          â”‚
â”‚  â”œâ”€ Confidence-based retraction                       â”‚
â”‚  â””â”€ Quality checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                                 â”‚                      â”‚
â”‚  Phase 2: CLINICAL INTELLIGENCE â”‚                      â”‚
â”‚  â”œâ”€ Timeline construction      â”‚                      â”‚
â”‚  â”œâ”€ Treatment response analysisâ”‚                      â”‚
â”‚  â”œâ”€ Functional evolution       â”‚                      â”‚
â”‚  â””â”€ Quality checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚
â”‚                                 â”‚                      â”‚
â”‚  Phase 3: NARRATIVE GENERATION  â”‚                      â”‚
â”‚  â”œâ”€ Template-guided structure  â”œâ”€ LLM Quality         â”‚
â”‚  â”œâ”€ LLM-enhanced content       â”‚  Governance          â”‚
â”‚  â”œâ”€ Medical writing style      â”‚  (Continuous)        â”‚
â”‚  â””â”€ Quality checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚
â”‚                                 â”‚                      â”‚
â”‚  Phase 4: QUALITY VALIDATION    â”‚                      â”‚
â”‚  â”œâ”€ 6-dimension assessment â”€â”€â”€â”€â”˜                      â”‚
â”‚  â”œâ”€ Comprehensive review                              â”‚
â”‚  â”œâ”€ Refinement decision                               â”‚
â”‚  â””â”€ Iteration (if needed, max 2x)                     â”‚
â”‚                                                        â”‚
â”‚  OUTPUT: High-Quality Discharge Summary               â”‚
â”‚  âœ… Complete                                          â”‚
â”‚  âœ… Accurate                                          â”‚
â”‚  âœ… Consistent                                        â”‚
â”‚  âœ… Professional                                      â”‚
â”‚  âœ… Clinically Excellent                             â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quality Standards

#### Minimum Quality Thresholds

```javascript
const QUALITY_STANDARDS = {
  production: {
    overall: 0.85,      // Overall quality score
    completeness: 0.90,  // All sections present and detailed
    accuracy: 0.95,      // Verified against source
    consistency: 0.90,   // Internally coherent
    narrativeQuality: 0.85,  // Professional writing
    specificity: 0.80,   // Precise details
    timeliness: 0.75     // Efficient processing
  },
  
  clinical: {
    overall: 0.90,      // Higher standard for clinical use
    completeness: 0.95,
    accuracy: 0.98,
    consistency: 0.95,
    narrativeQuality: 0.90,
    specificity: 0.85,
    timeliness: 0.70    // Quality over speed
  }
};
```

### Quality Assurance Mechanisms

#### 1. Pre-Generation Validation

```javascript
/**
 * Validate input quality before generation
 */
async function preGenerationValidation(clinicalNotes) {
  const validation = await llm.validateInput({
    notes: clinicalNotes,
    task: 'input_validation',
    instructions: `
      Assess input quality:
      
      1. SUFFICIENCY:
         - Is there enough information for a complete summary?
         - What critical data is missing?
         - Can a quality summary be generated?
      
      2. CLARITY:
         - Is the documentation clear and readable?
         - Are there significant ambiguities?
         - Is medical terminology used correctly?
      
      3. COMPLETENESS:
         - Are dates documented?
         - Is pathology information present?
         - Are procedures described?
         - Is discharge planning addressed?
      
      4. RECOMMENDATION:
         - Proceed with generation?
         - Request additional information?
         - Flag for manual review?
      
      Return: { 
        proceed: boolean, 
        confidence: number,
        missingCritical: [],
        warnings: []
      }
    `
  });
  
  if (!validation.proceed) {
    return {
      canGenerate: false,
      reason: 'Insufficient input quality',
      details: validation
    };
  }
  
  return { canGenerate: true };
}
```

#### 2. Post-Generation Review

```javascript
/**
 * Comprehensive post-generation quality review
 */
async function postGenerationReview(summary, extractedData, sourceNotes) {
  const review = await llm.comprehensiveReview({
    summary: summary,
    extracted: extractedData,
    source: sourceNotes,
    task: 'final_quality_review',
    instructions: `
      Perform final quality review as if you are a senior physician:
      
      1. CLINICAL ACCURACY:
         - Are diagnoses correctly stated?
         - Are procedures accurately described?
         - Are medications and dosages correct?
         - Are follow-up plans appropriate?
      
      2. COMPLETENESS:
         - Would this summary be sufficient for receiving provider?
         - Is any critical information missing?
         - Are all relevant complications documented?
      
      3. PROFESSIONALISM:
         - Is the writing professional and clear?
         - Are medical terms used appropriately?
         - Is the narrative easy to follow?
      
      4. PATIENT SAFETY:
         - Are there any safety concerns?
         - Are medication interactions noted?
         - Are warning signs documented?
      
      5. MEDICOLEGAL:
         - Is documentation sufficient?
         - Are complications and management documented?
         - Is informed consent referenced where relevant?
      
      Rate overall readiness for clinical use:
      - APPROVED: Ready for use
      - APPROVED_WITH_MINOR_EDITS: Nearly ready, minor fixes needed
      - NEEDS_REVISION: Significant issues require refinement
      - REJECT: Critical issues, do not use
      
      Provide detailed feedback for each category.
    `
  });
  
  return {
    status: review.status,
    readyForClinicalUse: review.status.startsWith('APPROVED'),
    feedback: review.feedback,
    requiredChanges: review.requiredChanges,
    recommendations: review.recommendations
  };
}
```

#### 3. Continuous Improvement

```javascript
/**
 * Learn from quality metrics to improve future generations
 */
async function continuousQualityImprovement(summary, qualityMetrics, userFeedback) {
  // LLM analyzes what worked and what didn't
  const insights = await llm.analyzeQualityPatterns({
    summary: anonymize(summary),
    metrics: qualityMetrics,
    feedback: userFeedback,
    task: 'quality_learning',
    instructions: `
      Analyze this case for quality improvement insights:
      
      1. SUCCESSES:
         - What aspects scored well?
         - What techniques were effective?
         - What should be replicated?
      
      2. FAILURES:
         - What aspects scored poorly?
         - What techniques were ineffective?
         - What should be avoided?
      
      3. PATTERNS:
         - Are there recurring issues?
         - What correlates with high quality?
         - What correlates with low quality?
      
      4. IMPROVEMENTS:
         - What specific changes would improve quality?
         - What new patterns should be learned?
         - What processes need adjustment?
      
      Generate actionable insights for system improvement.
    `
  });
  
  // Store insights for future reference
  await knowledgeBase.storeQualityInsights(insights);
  
  // Update generation strategies
  await updateGenerationStrategies(insights);
}
```

### Quality Metrics Dashboard

**File:** `src/components/ModelSelector.jsx`

The system provides real-time quality monitoring:

```javascript
{
  currentSummary: {
    overall: 0.92,
    dimensions: {
      completeness: { score: 0.95, rating: 'Excellent' },
      accuracy: { score: 0.96, rating: 'Excellent' },
      consistency: { score: 0.89, rating: 'Good' },
      narrativeQuality: { score: 0.91, rating: 'Excellent' },
      specificity: { score: 0.85, rating: 'Good' },
      timeliness: { score: 0.78, rating: 'Acceptable' }
    }
  },
  
  historicalTrends: {
    averageQuality: 0.87,
    qualityTrend: 'improving',  // improving, stable, declining
    bestDimensions: ['accuracy', 'completeness'],
    needsImprovement: ['timeliness']
  },
  
  recommendations: [
    'Current summary exceeds quality standards',
    'Consider review of consistency in date references',
    'Processing time within acceptable range'
  ]
}
```

---

## Multi-Provider Architecture

### Provider Selection and Fallback

**File:** `src/services/llmService.js`

The system supports multiple LLM providers with intelligent selection:

```javascript
export const PREMIUM_MODELS = [
  {
    id: 'claude-sonnet-3.5',
    name: 'Claude 3.5 Sonnet',
    provider: 'anthropic',
    costPer1MInput: 3.00,
    costPer1MOutput: 15.00,
    contextWindow: 200000,
    speed: 'medium',
    quality: 'excellent',
    recommended: true,
    bestFor: ['extraction', 'narrative', 'quality_assessment']
  },
  {
    id: 'gpt-4o',
    name: 'GPT-4o',
    provider: 'openai',
    costPer1MInput: 2.50,
    costPer1MOutput: 10.00,
    contextWindow: 128000,
    speed: 'fast',
    quality: 'excellent',
    recommended: true,
    bestFor: ['medical_reasoning', 'complex_cases']
  },
  {
    id: 'gemini-1.5-pro',
    name: 'Gemini 1.5 Pro',
    provider: 'google',
    costPer1MInput: 1.25,
    costPer1MOutput: 5.00,
    contextWindow: 2000000,
    speed: 'fast',
    quality: 'very_good',
    recommended: true,
    bestFor: ['large_context', 'cost_efficiency']
  }
];
```

### Automatic Fallback Strategy

```javascript
/**
 * Intelligent fallback with quality preservation
 */
async function callLLMWithFallback(prompt, options) {
  const primaryModel = getSelectedModel();
  const fallbackModels = getFallbackModels(primaryModel);
  
  console.log(`[LLM] ğŸ¯ Primary: ${primaryModel.name} for ${options.task}`);
  
  try {
    const result = await callLLM(prompt, { ...options, model: primaryModel });
    console.log(`[LLM] âœ… Success with ${primaryModel.name}`);
    return result;
    
  } catch (primaryError) {
    console.warn(`[LLM] âŒ Failed with ${primaryModel.name}:`, primaryError.message);
    
    // Try fallback models
    for (const fallbackModel of fallbackModels) {
      console.log(`[LLM] ğŸ”„ Fallback: ${fallbackModel.name} for ${options.task}`);
      
      try {
        const result = await callLLM(prompt, { ...options, model: fallbackModel });
        console.log(`[LLM] âœ… Success with ${fallbackModel.name} (fallback)`);
        
        // Log fallback event for quality tracking
        trackFallbackEvent(primaryModel, fallbackModel, options.task);
        
        return result;
        
      } catch (fallbackError) {
        console.warn(`[LLM] âŒ Failed with ${fallbackModel.name}:`, fallbackError.message);
        continue;
      }
    }
    
    // All providers failed
    throw new Error('All LLM providers failed. Please check API keys and credits.');
  }
}
```

### Task-Specific Provider Optimization

```javascript
/**
 * Select optimal provider for specific tasks
 */
function getOptimalProvider(task) {
  const taskOptimization = {
    'data_extraction': {
      primary: 'claude-sonnet-3.5',  // Best at structured extraction
      fallback: ['gpt-4o', 'gemini-1.5-pro']
    },
    
    'narrative_generation': {
      primary: 'claude-sonnet-3.5',  // Best at narrative
      fallback: ['gpt-4o', 'gemini-1.5-pro']
    },
    
    'quality_assessment': {
      primary: 'gpt-4o',  // Best at reasoning
      fallback: ['claude-sonnet-3.5', 'gemini-1.5-pro']
    },
    
    'medical_reasoning': {
      primary: 'gpt-4o',  // Strong medical knowledge
      fallback: ['claude-sonnet-3.5', 'gemini-1.5-pro']
    },
    
    'pattern_learning': {
      primary: 'claude-sonnet-3.5',  // Best at pattern recognition
      fallback: ['gpt-4o', 'gemini-1.5-pro']
    }
  };
  
  return taskOptimization[task] || taskOptimization['data_extraction'];
}
```

---

## Intelligent Feedback Loops

### Continuous Learning Architecture

The LLM system implements multiple feedback loops for continuous improvement:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INTELLIGENT FEEDBACK LOOPS                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  Loop 1: QUALITY-DRIVEN REFINEMENT                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Quality Assessment                 â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Score < Threshold?                 â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Refinement Strategy                â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Re-Extract or Re-Generate          â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Re-Assess Quality                  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â”‚  Loop 2: USER CORRECTION LEARNING                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ User Makes Correction              â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ LLM Analyzes Correction            â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Extract Pattern                    â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Store in Knowledge Base            â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Apply in Future Extractions        â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â”‚  Loop 3: COMPOSITION LEARNING                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Import Completed Summary           â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ LLM Analyzes Style & Structure     â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Extract Composition Patterns       â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Update Templates & Style           â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Apply in Future Generation         â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â”‚  Loop 4: PERFORMANCE MONITORING                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Track Provider Performance         â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Analyze Success Rates              â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Adjust Fallback Strategy           â”‚               â”‚
â”‚  â”‚         â†“                           â”‚               â”‚
â”‚  â”‚ Optimize Provider Selection        â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Examples

#### Loop 1: Quality-Driven Refinement

**File:** `src/services/summaryOrchestrator.js`

```javascript
async function orchestrateWithQualityFeedback(notes, options) {
  let iteration = 0;
  let qualityMet = false;
  let result = null;
  
  while (!qualityMet && iteration < 2) {
    iteration++;
    console.log(`[Orchestrator] Iteration ${iteration}`);
    
    // Generate summary
    result = await generateSummary(notes, options);
    
    // Assess quality
    const quality = await llmGovernedQualityAssessment(
      result.extractedData,
      result.narrative,
      notes
    );
    
    qualityMet = quality.overall.score >= options.qualityThreshold;
    
    if (!qualityMet && iteration < 2) {
      console.log(`[Orchestrator] Quality ${quality.overall.score} below threshold ${options.qualityThreshold}`);
      
      // Get refinement strategy from LLM
      const refinement = await intelligentRefinement(result, quality);
      
      // Apply refinements
      options = {
        ...options,
        refinementFocus: refinement.strategy,
        previousAttempt: result,
        qualityGaps: quality.actionable.criticalFixes
      };
    }
  }
  
  return {
    ...result,
    qualityMetrics: quality,
    iterations: iteration,
    qualityThresholdMet: qualityMet
  };
}
```

#### Loop 2: User Correction Learning

**File:** `src/services/ml/learningEngine.js`

```javascript
export async function learnFromCorrection(original, corrected, context) {
  console.log('[Learning Engine] Processing user correction...');
  
  // Anonymize PHI before learning
  const safeOriginal = await anonymizePHI(original);
  const safeCorrected = await anonymizePHI(corrected);
  const safeContext = await anonymizePHI(context);
  
  // LLM analyzes the correction
  const learningInsights = await llm.analyzeCorrection({
    original: safeOriginal,
    corrected: safeCorrected,
    context: safeContext,
    task: 'correction_learning',
    instructions: `
      A user corrected extracted data. Learn from this correction:
      
      Original: ${JSON.stringify(safeOriginal, null, 2)}
      Corrected: ${JSON.stringify(safeCorrected, null, 2)}
      Context: ${safeContext}
      
      Analyze:
      1. What was the error in original extraction?
      2. What pattern should have been recognized?
      3. What context clues were missed?
      4. How can this be prevented in future?
      
      Generate:
      - Extraction pattern for this scenario
      - Context rules for pattern application
      - Confidence score for pattern (0.6-0.8 for new patterns)
      - Boundary conditions
      
      Return structured learning that can be applied automatically.
    `
  });
  
  // Store learned pattern
  await knowledgeBase.storeLearnedPattern({
    pattern: learningInsights.pattern,
    contextRules: learningInsights.contextRules,
    confidence: learningInsights.confidence,
    source: 'user_correction',
    verified: false,
    timestamp: new Date().toISOString()
  });
  
  console.log('[Learning Engine] âœ… Pattern learned and stored');
  
  return learningInsights;
}
```

---

## Quality Governance Framework

### Governance Principles

1. **Quality First**: Quality is prioritized over speed or cost
2. **Transparency**: All quality assessments are explainable
3. **Continuous Monitoring**: Quality is assessed at every phase
4. **Adaptive Standards**: Quality thresholds adapt to use case
5. **Learning Enabled**: System improves from quality metrics

### Quality Governance Workflow

```javascript
/**
 * Comprehensive Quality Governance
 */
class QualityGovernor {
  constructor() {
    this.llm = llmService;
    this.standards = QUALITY_STANDARDS;
    this.history = [];
  }
  
  /**
   * Phase 1: Pre-Generation Assessment
   */
  async assessInputQuality(clinicalNotes) {
    const assessment = await this.llm.assessInput({
      notes: clinicalNotes,
      task: 'input_quality_assessment'
    });
    
    this.history.push({
      phase: 'input',
      timestamp: new Date(),
      assessment: assessment
    });
    
    return assessment;
  }
  
  /**
   * Phase 2: Extraction Quality Check
   */
  async checkExtractionQuality(extractedData, sourceNotes) {
    const check = await this.llm.checkExtraction({
      extracted: extractedData,
      source: sourceNotes,
      standards: this.standards
    });
    
    this.history.push({
      phase: 'extraction',
      timestamp: new Date(),
      check: check
    });
    
    if (check.shouldRefine) {
      console.log('[Quality Governor] Extraction refinement recommended');
      return { status: 'refine', issues: check.issues };
    }
    
    return { status: 'pass' };
  }
  
  /**
   * Phase 3: Narrative Quality Assessment
   */
  async assessNarrativeQuality(narrative, extractedData, sourceNotes) {
    const assessment = await llmGovernedQualityAssessment(
      extractedData,
      narrative,
      sourceNotes
    );
    
    this.history.push({
      phase: 'narrative',
      timestamp: new Date(),
      assessment: assessment
    });
    
    return assessment;
  }
  
  /**
   * Phase 4: Final Approval Decision
   */
  async finalApprovalDecision(completeResult) {
    const decision = await this.llm.makeApprovalDecision({
      result: completeResult,
      qualityHistory: this.history,
      standards: this.standards,
      task: 'final_approval'
    });
    
    return {
      approved: decision.approved,
      confidence: decision.confidence,
      conditions: decision.conditions,
      recommendations: decision.recommendations
    };
  }
  
  /**
   * Generate Quality Report
   */
  generateQualityReport() {
    return {
      phases: this.history,
      overallTrend: this.analyzeQualityTrend(),
      recommendations: this.generateRecommendations(),
      timestamp: new Date().toISOString()
    };
  }
}
```

---

## Performance Metrics and Monitoring

### Real-Time Performance Tracking

**File:** `src/services/llmService.js`

```javascript
/**
 * Performance metrics tracked per call
 */
const PERFORMANCE_METRICS = {
  byProvider: {
    'Claude 3.5 Sonnet': {
      totalCalls: 0,
      successfulCalls: 0,
      failedCalls: 0,
      avgDuration: 0,
      avgCost: 0,
      avgInputTokens: 0,
      avgOutputTokens: 0,
      successRate: 0
    },
    // ... other providers
  },
  
  byTask: {
    'data_extraction': {
      totalCalls: 0,
      avgQualityScore: 0,
      avgDuration: 0,
      successRate: 0
    },
    // ... other tasks
  }
};
```

### Cost Tracking and Optimization

```javascript
/**
 * Cost tracking per API call
 */
function recordCost(provider, task, inputTokens, outputTokens, cost, duration, success, error) {
  const tracking = getCostTracking();
  
  // Update totals
  tracking.totalCost += cost;
  tracking.totalCalls++;
  
  // Update per-provider tracking
  if (!tracking.byProvider[provider]) {
    tracking.byProvider[provider] = {
      cost: 0,
      calls: 0,
      tokens: 0
    };
  }
  
  tracking.byProvider[provider].cost += cost;
  tracking.byProvider[provider].calls++;
  tracking.byProvider[provider].tokens += (inputTokens + outputTokens);
  
  // Update per-task tracking
  if (!tracking.byTask[task]) {
    tracking.byTask[task] = {
      cost: 0,
      calls: 0,
      tokens: 0
    };
  }
  
  tracking.byTask[task].cost += cost;
  tracking.byTask[task].calls++;
  tracking.byTask[task].tokens += (inputTokens + outputTokens);
  
  // Store in history
  tracking.history.push({
    timestamp: new Date().toISOString(),
    provider,
    task,
    inputTokens,
    outputTokens,
    cost,
    duration,
    success,
    error: error ? error.message : null
  });
  
  saveCostTracking(tracking);
  
  console.log(`[LLM Cost] $${cost.toFixed(4)} | ${provider} | ${task} | ${duration}ms`);
}
```

### Dashboard Metrics

The system provides comprehensive dashboards showing:

1. **Cost Metrics**
   - Total cost across all providers
   - Cost breakdown by provider
   - Cost breakdown by task
   - Average cost per call
   - Cost trends over time

2. **Performance Metrics**
   - Average duration per provider
   - Average duration per task
   - Success rates
   - Failure patterns
   - Quality scores

3. **Quality Metrics**
   - Overall quality trends
   - Dimension-specific trends
   - Common quality issues
   - Improvement patterns

4. **Learning Metrics**
   - Patterns learned
   - Pattern success rates
   - Correction frequency
   - Quality improvement over time

---

## Summary

The **LLM as AI Context-Aware Engine** transforms the Discharge Summary Generator from a rigid, rule-based system into an **intelligent, adaptive, and continuously improving** medical documentation platform.

### Key Innovations

1. **Flexible Pattern-Based Retraction**
   - LLM validates and enhances pattern matches
   - Context-aware interpretation
   - Confidence-based retraction
   - Continuous pattern learning

2. **Template Enhancement**
   - Templates provide structure
   - LLM provides intelligence
   - Dynamic adaptation to case complexity
   - Learned institutional preferences

3. **Quality Governance**
   - 6-dimension quality assessment
   - Continuous quality monitoring
   - Quality-driven refinement loops
   - Explainable quality decisions

4. **High-Quality Output**
   - Quality as primary objective
   - Multiple validation checkpoints
   - Intelligent refinement
   - Clinical excellence standards

### Benefits

- **Flexibility**: Adapts to clinical variability
- **Accuracy**: Validates and ensures correctness
- **Quality**: Maintains high standards
- **Learning**: Improves continuously
- **Reliability**: Multi-provider fallback
- **Transparency**: Explainable decisions

---

**Document Maintained By:** DCS Development Team  
**Next Review:** January 2026  
**Questions/Issues:** See GitHub Issues  
